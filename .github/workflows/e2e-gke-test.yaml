name: GKE llm-d Inference Scheduling Test

on:
  # This is for testing only, will remove later
  pull_request:
    types: [opened, synchronize, reopened]

  issue_comment:
    # Runs with a PR comment /run-e2e or /run-e2e-istio
    types: [created]
#  schedule: # TODO: enable once validated functional
#    - cron: '0 8 * * *'  # 4AM Eastern (08:00 UTC)
  workflow_dispatch:
    inputs:
      pr_or_branch:
        description: 'Pull-request number or branch name to test'
        required: true
        default: 'main'
        type: string

permissions:
  contents: read

jobs:
  deploy_and_validate:
    if: >
      github.event_name == 'pull_request' ||
      github.event_name == 'workflow_dispatch' ||
      (
        github.event_name == 'issue_comment' &&
        github.event.issue.pull_request &&
        (
          contains(github.event.comment.body, '/run-gke-e2e')
        ) &&
        (
          github.event.comment.author_association == 'OWNER' ||
          github.event.comment.author_association == 'MEMBER' ||
          github.event.comment.author_association == 'COLLABORATOR'
        )
      )
    runs-on: ubuntu-latest

    env:
      GCP_PROJECT_ID: llm-d-scale
      GKE_CLUSTER_NAME: llm-d-e2e-us-east5
      GKE_CLUSTER_ZONE: us-east5
      NAMESPACE: llm-d-inference-scheduling
      GATEWAY: gke-l7-regional-external-managed
      PR_OR_BRANCH: ${{ github.event.inputs.pr_or_branch }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@b7593ed2efd1c1617e1b0254da33b86225adb2a5
        with:
          credentials_json: ${{ secrets.GKE_SA_KEY }}

      - name: Set up gcloud CLI and kubectl
        uses: google-github-actions/setup-gcloud@cb1e50a9932213ecece00a606661ae9ca44f3397
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}
          install_components: 'kubectl,gke-gcloud-auth-plugin'

      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials "${{ env.GKE_CLUSTER_NAME }}" --zone "${{ env.GKE_CLUSTER_ZONE }}"

      - name: Run llmd-infra-installer.sh
        run: |
          cd llm-d-infra/quickstart
          ./llmd-infra-installer.sh --namespace "${{ env.NAMESPACE }}" -r infra-inference-scheduling --gateway "${{ env.GATEWAY }}" --disable-metrics-collection
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}

      - name: Deploy model service and GIE with Helmfile
        run: |
          cd llm-d-infra/quickstart/examples/inference-scheduling
          helmfile apply \
            --namespace "${{ env.NAMESPACE }}" \
            --selector managedBy=helmfile \
            -f gke.helmfile.yaml --skip-diff-on-install

      - name: Wait for all pods to be ready
        run: |
          kubectl wait pod --for=condition=Ready --all -n "${{ env.NAMESPACE }}" --timeout=15m

      - name: Verify installation and run inference tests
        run: |
          GATEWAY_NAME=infra-inference-scheduling-inference-gateway

          # Wait for the Gateway IP to be assigned
          sleep 120

          # Get the Gateway IP
          IP=$(kubectl get gateway/${GATEWAY_NAME} -n "${{ env.NAMESPACE }}" -o jsonpath='{.status.addresses[0].value}')
          PORT=80

          if [[ -z "$IP" ]]; then
            echo "❌ Gateway IP not found."
            exit 1
          fi

          echo "Gateway IP: $IP"

          echo "➡️ Verifying /v1/models endpoint"
          curl_models_output=$(curl http://${IP}:${PORT}/v1/models -H "Content-Type: application/json" | jq .)
          echo "Output: $curl_models_output"
          if [[ "$curl_models_output" =~ "Qwen/Qwen3-0.6B" ]]; then
            echo "✅ /v1/models endpoint check passed."
          else
            echo "❌ /v1/models endpoint check failed."
            exit 1
          fi

          # Send a simple inference request to verify end-to-end functionality
          response=$(curl -s -X POST "http://${gateway_ip}/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d '{"messages":[{"role":"system","content":"You are an AI assistant."},{"role":"user","content":"What is the capital of France?"}],"max_tokens":15}')

          echo "Inference Response: $response"

          if [[ "$response" =~ "Paris" ]]; then
            echo "✅ End-to-end test successful!"
          else
            echo "❌ End-to-end test failed. Response did not contain 'Paris'."
            exit 1
          fi

      - name: Cleanup deployment
        if: always()
        run: |
          cd llm-d-infra/quickstart/examples/inference-scheduling
          helmfile destroy --namespace "${{ env.NAMESPACE }}" --selector managedBy=helmfile

          helm uninstall infra-inference-scheduling --namespace "${{ env.NAMESPACE }}"
